{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13=02-2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaD5XBr58BIZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# LOSSES\n",
    "# ============================================================\n",
    "\n",
    "def kl_loss(mean, logvar):\n",
    "    return -0.5 * torch.sum(\n",
    "        1 + logvar - mean.pow(2) - logvar.exp(),\n",
    "        dim=[1, 2, 3]\n",
    "    ).mean()\n",
    "\n",
    "def recon_loss(pred, target):\n",
    "    return F.l1_loss(pred, target)\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "def show_grid(tensor, title=\"\"):\n",
    "    tensor = (tensor + 1) / 2\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "\n",
    "    grid = make_grid(tensor, nrow=8)\n",
    "    grid = grid.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.squeeze(), cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# STOCHASTICITY CHECK\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_latent_stochasticity(vae, loader):\n",
    "    vae.eval()\n",
    "    batch = next(iter(loader))\n",
    "    x = batch[\"image\"].to(device)\n",
    "\n",
    "    mean, logvar = vae.encode(x)\n",
    "\n",
    "    z1 = vae.reparameterize(mean, logvar)\n",
    "    z2 = vae.reparameterize(mean, logvar)\n",
    "\n",
    "    diff = torch.mean(torch.abs(z1 - z2))\n",
    "    print(\"Latent stochastic difference:\", diff.item())\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN\n",
    "# ============================================================\n",
    "\n",
    "def train_vae(vae, train_loader, val_loader,\n",
    "              epochs=150, lr=1e-4,\n",
    "              out_dir=\"./vae_outputs\"):\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    preview_dir = os.path.join(out_dir, \"previews\")\n",
    "    os.makedirs(preview_dir, exist_ok=True)\n",
    "\n",
    "    vae = vae.to(device)\n",
    "\n",
    "    ckpt_file = os.path.join(out_dir, \"resume_checkpoint.pt\")\n",
    "    best_model = os.path.join(out_dir, \"vae_best.pth\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(vae.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "    start_epoch = 1\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    # Resume training\n",
    "    if os.path.exists(ckpt_file):\n",
    "        ckpt = torch.load(ckpt_file, map_location=device)\n",
    "        vae.load_state_dict(ckpt[\"model\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        if \"scaler\" in ckpt:\n",
    "            scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        best_val = ckpt[\"best_val\"]\n",
    "        print(\"Resuming from epoch\", start_epoch)\n",
    "\n",
    "    preview = next(iter(val_loader))[\"image\"]\n",
    "    preview = preview[:min(8, preview.size(0))].to(device)\n",
    "\n",
    "    # ========================================================\n",
    "    # TRAIN LOOP\n",
    "    # ========================================================\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "\n",
    "        # KL warmup\n",
    "        beta = 0.01 * min(1.0, epoch / 100)\n",
    "\n",
    "        vae.train()\n",
    "        total_recon = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[TRAIN] Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for batch in pbar:\n",
    "            img = batch[\"image\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                recon, mean, logvar = vae(img)\n",
    "                loss_r = recon_loss(recon, img)\n",
    "                loss_k = kl_loss(mean, logvar)\n",
    "                loss = 10 * loss_r + beta * loss_k\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_recon += loss_r.item()\n",
    "\n",
    "            pbar.set_postfix(L1=f\"{loss_r.item():.4f}\",\n",
    "                             KL=f\"{loss_k.item():.4f}\",\n",
    "                             beta=f\"{beta:.5f}\")\n",
    "\n",
    "        total_recon /= len(train_loader)\n",
    "\n",
    "        # ================= VALIDATION (deterministic) =================\n",
    "\n",
    "        vae.eval()\n",
    "        val_recon = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                img = batch[\"image\"].to(device)\n",
    "\n",
    "                mean, logvar = vae.encode(img)\n",
    "                recon = vae.decode(mean)\n",
    "\n",
    "                val_recon += recon_loss(recon, img).item()\n",
    "\n",
    "        val_recon /= len(val_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}\")\n",
    "        print(f\"Train L1: {total_recon:.4f} | Val L1: {val_recon:.4f}\")\n",
    "\n",
    "        # ================= PREVIEW (deterministic) =================\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = vae.encode(preview)\n",
    "            recon = vae.decode(mean)\n",
    "\n",
    "        vis = torch.cat([preview, recon], dim=0)\n",
    "\n",
    "        save_path = os.path.join(preview_dir, f\"epoch_{epoch}.png\")\n",
    "        save_image((vis + 1) / 2, save_path, nrow=8)\n",
    "\n",
    "        show_grid(vis, title=f\"Epoch {epoch}\")\n",
    "\n",
    "        # ================= SAVE =================\n",
    "\n",
    "        if val_recon < best_val:\n",
    "            best_val = val_recon\n",
    "            torch.save(vae.state_dict(), best_model)\n",
    "            print(\"New best model saved\")\n",
    "\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": vae.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict(),\n",
    "            \"best_val\": best_val\n",
    "        }, ckpt_file)\n",
    "\n",
    "        print(\"Checkpoint saved\")\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_vae(vae, test_loader,\n",
    "             ckpt_path=\"./vae_outputs/vae_best.pth\"):\n",
    "\n",
    "    vae.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    vae.eval()\n",
    "\n",
    "    batch = next(iter(test_loader))\n",
    "    img = batch[\"image\"][:min(8, batch[\"image\"].size(0))].to(device)\n",
    "\n",
    "    mean, logvar = vae.encode(img)\n",
    "    recon = vae.decode(mean)\n",
    "\n",
    "    show_grid(torch.cat([img, recon], dim=0),\n",
    "              title=\"Test Reconstructions\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "\n",
    "vae = VAE(z_channels=8).to(device)\n",
    "\n",
    "train_vae(vae, train_loader, val_loader)\n",
    "\n",
    "check_latent_stochasticity(vae, train_loader)\n",
    "\n",
    "test_vae(vae, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14-02-2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import lpips\n",
    "\n",
    "# ============================================================\n",
    "# DEVICE\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# SAFE NORMALIZATION\n",
    "# ============================================================\n",
    "\n",
    "def normalize_batch(x):\n",
    "    if x.max() > 1.5:\n",
    "        x = x / 127.5 - 1\n",
    "    elif x.min() >= 0:\n",
    "        x = x * 2 - 1\n",
    "    return x.clamp(-1, 1)\n",
    "\n",
    "# ============================================================\n",
    "# PERCEPTUAL LOSS\n",
    "# ============================================================\n",
    "\n",
    "lpips_loss = lpips.LPIPS(net='vgg').to(device)\n",
    "lpips_loss.eval()\n",
    "\n",
    "for p in lpips_loss.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def perceptual_loss(pred, target):\n",
    "    pred3 = pred.repeat(1,3,1,1)\n",
    "    target3 = target.repeat(1,3,1,1)\n",
    "    return lpips_loss(pred3, target3).mean()\n",
    "\n",
    "# ============================================================\n",
    "# LOSSES\n",
    "# ============================================================\n",
    "\n",
    "def kl_loss(mean, logvar):\n",
    "    logvar = torch.clamp(logvar, -30, 20)\n",
    "    return -0.5 * torch.sum(\n",
    "        1 + logvar - mean.pow(2) - logvar.exp(),\n",
    "        dim=[1,2,3]\n",
    "    ).mean()\n",
    "\n",
    "def recon_loss(pred, target):\n",
    "    return F.l1_loss(pred, target)\n",
    "\n",
    "# ============================================================\n",
    "# PREVIEW STRETCH\n",
    "# ============================================================\n",
    "\n",
    "def stretch(x):\n",
    "    x = x - x.min()\n",
    "    x = x / (x.max() + 1e-8)\n",
    "    return x * 2 - 1\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY GRID\n",
    "# ============================================================\n",
    "\n",
    "def show_grid(tensor, title=\"\"):\n",
    "    tensor = (tensor + 1) / 2\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "\n",
    "    grid = make_grid(tensor, nrow=4)\n",
    "    grid = grid.cpu().permute(1,2,0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(grid.squeeze(), cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# TRAIN FUNCTION\n",
    "\n",
    "\n",
    "def train_vae(vae, train_loader, val_loader,\n",
    "              epochs=150, lr=1e-4,\n",
    "              out_dir=\"./vae_outputs\"):\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    preview_dir = os.path.join(out_dir, \"previews\")\n",
    "    os.makedirs(preview_dir, exist_ok=True)\n",
    "\n",
    "    vae = vae.to(device)\n",
    "    optimizer = torch.optim.AdamW(vae.parameters(), lr=lr)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type==\"cuda\"))\n",
    "\n",
    "    preview = normalize_batch(\n",
    "        next(iter(val_loader))[\"image\"][:4].to(device)\n",
    "    )\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        beta = 1e-4\n",
    "\n",
    "        # ✅ perceptual gentle warmup\n",
    "        perc_weight = min(0.02, epoch * 0.005)\n",
    "\n",
    "        vae.train()\n",
    "        total_recon = 0\n",
    "\n",
    "        pbar = tqdm(train_loader,\n",
    "                    desc=f\"[TRAIN] Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for batch in pbar:\n",
    "            img = normalize_batch(batch[\"image\"].to(device))\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\",\n",
    "                                    enabled=(device.type==\"cuda\")):\n",
    "\n",
    "                recon, mean, logvar = vae(img)\n",
    "\n",
    "                loss_l1 = recon_loss(recon, img)\n",
    "                loss_p  = perceptual_loss(recon, img)\n",
    "                loss_k  = kl_loss(mean, logvar)\n",
    "\n",
    "                loss = loss_l1 + perc_weight*loss_p + beta*loss_k\n",
    "\n",
    "            if not torch.isfinite(loss):\n",
    "                print(\"NaN detected — stopping\")\n",
    "                return\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_recon += loss_l1.item()\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                L1=f\"{loss_l1.item():.4f}\",\n",
    "                KL=f\"{loss_k.item():.4f}\",\n",
    "                pw=f\"{perc_weight:.3f}\"\n",
    "            )\n",
    "\n",
    "        total_recon /= len(train_loader)\n",
    "\n",
    "        # ===== VALIDATION =====\n",
    "\n",
    "        vae.eval()\n",
    "        val_recon = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                img = normalize_batch(batch[\"image\"].to(device))\n",
    "                mean, logvar = vae.encode(img)\n",
    "                recon = vae.decode(mean)\n",
    "                val_recon += recon_loss(recon, img).item()\n",
    "\n",
    "        val_recon /= len(val_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}\")\n",
    "        print(f\"Train L1: {total_recon:.4f} | Val L1: {val_recon:.4f}\")\n",
    "\n",
    "        # ===== PREVIEW =====\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean, logvar = vae.encode(preview)\n",
    "            recon = vae.decode(mean)\n",
    "\n",
    "        vis = torch.cat([preview, stretch(recon)], dim=0)\n",
    "\n",
    "        save_image((vis+1)/2,\n",
    "                   os.path.join(preview_dir,\n",
    "                   f\"epoch_{epoch}.png\"),\n",
    "                   nrow=4)\n",
    "\n",
    "        show_grid(vis, title=f\"Epoch {epoch}\")\n",
    "\n",
    "        # ===== SAVE =====\n",
    "\n",
    "        if val_recon < best_val:\n",
    "            best_val = val_recon\n",
    "            torch.save(vae.state_dict(),\n",
    "                       os.path.join(out_dir,\n",
    "                       \"vae_best.pth\"))\n",
    "            print(\"New best model saved\")\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "\n",
    "\n",
    "vae = VAE(z_channels=16).to(device)\n",
    "train_vae(vae, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17-02-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "ROOT = \"/kaggle/input/datasets/rushikannan05/ccds-10k-daataset/CCDS_Split_10K\"\n",
    "\n",
    "TRAIN_PATH = os.path.join(ROOT, \"train\")\n",
    "VAL_PATH   = os.path.join(ROOT, \"val\")\n",
    "TEST_PATH  = os.path.join(ROOT, \"test\")\n",
    "\n",
    "VAE_PATH = \"/kaggle/input/models/rushikannan05/vae-best/pytorch/default/1/vae_best.pth\"\n",
    "INPUT_CHECKPOINT = \"/kaggle/input/models/rushikannan05/ldm-checkpoint/pytorch/default/1/ldm_checkpoint.pth\"\n",
    "\n",
    "OUT_DIR = \"/kaggle/working/LDM\"\n",
    "PREVIEW_DIR = os.path.join(OUT_DIR, \"previews\")\n",
    "BEST_MODEL_PATH = os.path.join(OUT_DIR, \"ldm_best.pth\")\n",
    "CHECKPOINT_PATH = os.path.join(OUT_DIR, \"ldm_checkpoint.pth\")\n",
    "\n",
    "os.makedirs(PREVIEW_DIR, exist_ok=True)\n",
    "\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, root, size=512):\n",
    "        self.paths = sorted(glob.glob(os.path.join(root, \"**/*.png\"), recursive=True))\n",
    "        self.size = size\n",
    "        print(f\"{root} → {len(self.paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = cv2.imread(self.paths[i], cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (self.size, self.size))\n",
    "        img = torch.from_numpy(img).float()/255\n",
    "        img = img.unsqueeze(0)*2 - 1\n",
    "        return {\"image\": img}\n",
    "\n",
    "train_loader = DataLoader(OCTDataset(TRAIN_PATH), batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(OCTDataset(VAL_PATH),   batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(OCTDataset(TEST_PATH),  batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "def norm_layer(ch):\n",
    "    return nn.GroupNorm(min(32, ch), ch)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(ch)\n",
    "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.norm2 = norm_layer(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(self.act(self.norm1(x)))\n",
    "        h = self.conv2(self.act(self.norm2(h)))\n",
    "        return x + h\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1)\n",
    "        self.norm = norm_layer(out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1)\n",
    "        self.norm = norm_layer(out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        b=64; z=16\n",
    "\n",
    "        self.conv_in = nn.Conv2d(1,b,3,1,1)\n",
    "        self.down1 = DownBlock(b,b*2); self.res1 = ResBlock(b*2)\n",
    "        self.down2 = DownBlock(b*2,b*4); self.res2 = ResBlock(b*4)\n",
    "        self.down3 = DownBlock(b*4,b*4); self.res3 = ResBlock(b*4)\n",
    "\n",
    "        self.to_stats = nn.Conv2d(b*4,z*2,3,1,1)\n",
    "        self.from_latent = nn.Conv2d(z,b*4,3,1,1)\n",
    "\n",
    "        self.res4 = ResBlock(b*4)\n",
    "        self.up1  = UpBlock(b*4,b*4)\n",
    "        self.res5 = ResBlock(b*4)\n",
    "        self.up2  = UpBlock(b*4,b*2)\n",
    "        self.res6 = ResBlock(b*2)\n",
    "        self.up3  = UpBlock(b*2,b)\n",
    "        self.res7 = ResBlock(b)\n",
    "\n",
    "        self.norm_out = norm_layer(b)\n",
    "        self.conv_out = nn.Conv2d(b,1,3,1,1)\n",
    "\n",
    "    def encode(self,x):\n",
    "        x=self.conv_in(x)\n",
    "        x=self.res1(self.down1(x))\n",
    "        x=self.res2(self.down2(x))\n",
    "        x=self.res3(self.down3(x))\n",
    "        m,l=torch.chunk(self.to_stats(x),2,1)\n",
    "        return m,l.clamp(-10,10)\n",
    "\n",
    "    def decode(self,z):\n",
    "        x=self.from_latent(z)\n",
    "        x=self.res4(x)\n",
    "        x=self.res5(self.up1(x))\n",
    "        x=self.res6(self.up2(x))\n",
    "        x=self.res7(self.up3(x))\n",
    "        return torch.tanh(self.conv_out(F.silu(self.norm_out(x))))\n",
    "\n",
    "vae = VAE().to(device)\n",
    "vae.load_state_dict(torch.load(VAE_PATH,map_location=device))\n",
    "vae.eval().requires_grad_(False)\n",
    "\n",
    "unet = UNet2DModel(\n",
    "    sample_size=64,\n",
    "    in_channels=16,\n",
    "    out_channels=16,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128,256,512,512),\n",
    "    down_block_types=(\"DownBlock2D\",\"AttnDownBlock2D\",\"AttnDownBlock2D\",\"AttnDownBlock2D\"),\n",
    "    up_block_types=(\"AttnUpBlock2D\",\"AttnUpBlock2D\",\"AttnUpBlock2D\",\"UpBlock2D\")\n",
    ").to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "opt = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "scaler = torch.amp.GradScaler(device.type)\n",
    "\n",
    "best=float(\"inf\")\n",
    "start=1\n",
    "\n",
    "if os.path.exists(INPUT_CHECKPOINT):\n",
    "    ck=torch.load(INPUT_CHECKPOINT,map_location=device)\n",
    "    unet.load_state_dict(ck[\"model\"])\n",
    "    opt.load_state_dict(ck[\"optimizer\"])\n",
    "    scaler.load_state_dict(ck[\"scaler\"])\n",
    "    best=ck[\"best_loss\"]\n",
    "    start=ck[\"epoch\"]+1\n",
    "    print(\"Resuming at epoch\",start)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader,name):\n",
    "    unet.eval(); total=0\n",
    "    for b in loader:\n",
    "        img=b[\"image\"].to(device)\n",
    "        m,l=vae.encode(img)\n",
    "        z=m+torch.randn_like(m)*torch.exp(0.5*l)\n",
    "        n=torch.randn_like(z)\n",
    "        t=torch.randint(0,1000,(z.size(0),),device=device)\n",
    "        xt=scheduler.add_noise(z,n,t)\n",
    "        pred=unet(xt,t).sample\n",
    "        total+=F.mse_loss(pred,n).item()\n",
    "    avg=total/len(loader)\n",
    "    print(f\"{name} Loss: {avg:.6f}\")\n",
    "    return avg\n",
    "\n",
    "for e in range(start,51):\n",
    "\n",
    "    total=0; unet.train()\n",
    "\n",
    "    for b in tqdm(train_loader,desc=f\"Epoch {e}\"):\n",
    "\n",
    "        img=b[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            m,l=vae.encode(img)\n",
    "            z=m+torch.randn_like(m)*torch.exp(0.5*l)\n",
    "\n",
    "        n=torch.randn_like(z)\n",
    "        t=torch.randint(0,1000,(z.size(0),),device=device)\n",
    "        xt=scheduler.add_noise(z,n,t)\n",
    "\n",
    "        with torch.amp.autocast(device.type):\n",
    "            pred=unet(xt,t).sample\n",
    "            loss=F.mse_loss(pred,n)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        total+=loss.item()\n",
    "\n",
    "    train_loss=total/len(train_loader)\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "    val_loss=evaluate(val_loader,\"VAL\")\n",
    "\n",
    "    if val_loss<best:\n",
    "        best=val_loss\n",
    "        torch.save(unet.state_dict(),BEST_MODEL_PATH)\n",
    "        print(\"⭐ Best model saved\")\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\":e,\n",
    "        \"model\":unet.state_dict(),\n",
    "        \"optimizer\":opt.state_dict(),\n",
    "        \"scaler\":scaler.state_dict(),\n",
    "        \"best_loss\":best\n",
    "    },CHECKPOINT_PATH)\n",
    "\n",
    "print(\"\\nFinal Test:\")\n",
    "evaluate(test_loader,\"TEST\")\n",
    "\n",
    "print(\"✅ Training complete\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPfsrhG0nvk0YHd9O8t8dV1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
