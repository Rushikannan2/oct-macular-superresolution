{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfsrhG0nvk0YHd9O8t8dV1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gaD5XBr58BIZ"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from torchvision.utils import make_grid, save_image\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ============================================================\n","# LOSSES\n","# ============================================================\n","\n","def kl_loss(mean, logvar):\n","    return -0.5 * torch.sum(\n","        1 + logvar - mean.pow(2) - logvar.exp(),\n","        dim=[1, 2, 3]\n","    ).mean()\n","\n","def recon_loss(pred, target):\n","    return F.l1_loss(pred, target)\n","\n","# ============================================================\n","# VISUALIZATION\n","# ============================================================\n","\n","def show_grid(tensor, title=\"\"):\n","    tensor = (tensor + 1) / 2\n","    tensor = torch.clamp(tensor, 0, 1)\n","\n","    grid = make_grid(tensor, nrow=8)\n","    grid = grid.cpu().permute(1, 2, 0).numpy()\n","\n","    plt.figure(figsize=(8, 8))\n","    plt.imshow(grid.squeeze(), cmap='gray')\n","    plt.title(title)\n","    plt.axis('off')\n","    plt.show()\n","\n","# ============================================================\n","# STOCHASTICITY CHECK\n","# ============================================================\n","\n","@torch.no_grad()\n","def check_latent_stochasticity(vae, loader):\n","    vae.eval()\n","    batch = next(iter(loader))\n","    x = batch[\"image\"].to(device)\n","\n","    mean, logvar = vae.encode(x)\n","\n","    z1 = vae.reparameterize(mean, logvar)\n","    z2 = vae.reparameterize(mean, logvar)\n","\n","    diff = torch.mean(torch.abs(z1 - z2))\n","    print(\"Latent stochastic difference:\", diff.item())\n","\n","# ============================================================\n","# TRAIN\n","# ============================================================\n","\n","def train_vae(vae, train_loader, val_loader,\n","              epochs=150, lr=1e-4,\n","              out_dir=\"./vae_outputs\"):\n","\n","    os.makedirs(out_dir, exist_ok=True)\n","    preview_dir = os.path.join(out_dir, \"previews\")\n","    os.makedirs(preview_dir, exist_ok=True)\n","\n","    vae = vae.to(device)\n","\n","    ckpt_file = os.path.join(out_dir, \"resume_checkpoint.pt\")\n","    best_model = os.path.join(out_dir, \"vae_best.pth\")\n","\n","    optimizer = torch.optim.AdamW(vae.parameters(), lr=lr)\n","    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n","\n","    start_epoch = 1\n","    best_val = float(\"inf\")\n","\n","    # Resume training\n","    if os.path.exists(ckpt_file):\n","        ckpt = torch.load(ckpt_file, map_location=device)\n","        vae.load_state_dict(ckpt[\"model\"])\n","        optimizer.load_state_dict(ckpt[\"optimizer\"])\n","        if \"scaler\" in ckpt:\n","            scaler.load_state_dict(ckpt[\"scaler\"])\n","        start_epoch = ckpt[\"epoch\"] + 1\n","        best_val = ckpt[\"best_val\"]\n","        print(\"Resuming from epoch\", start_epoch)\n","\n","    preview = next(iter(val_loader))[\"image\"]\n","    preview = preview[:min(8, preview.size(0))].to(device)\n","\n","    # ========================================================\n","    # TRAIN LOOP\n","    # ========================================================\n","\n","    for epoch in range(start_epoch, epochs + 1):\n","\n","        # KL warmup\n","        beta = 0.01 * min(1.0, epoch / 100)\n","\n","        vae.train()\n","        total_recon = 0\n","\n","        pbar = tqdm(train_loader, desc=f\"[TRAIN] Epoch {epoch}/{epochs}\")\n","\n","        for batch in pbar:\n","            img = batch[\"image\"].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n","                recon, mean, logvar = vae(img)\n","                loss_r = recon_loss(recon, img)\n","                loss_k = kl_loss(mean, logvar)\n","                loss = 10 * loss_r + beta * loss_k\n","\n","            scaler.scale(loss).backward()\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            total_recon += loss_r.item()\n","\n","            pbar.set_postfix(L1=f\"{loss_r.item():.4f}\",\n","                             KL=f\"{loss_k.item():.4f}\",\n","                             beta=f\"{beta:.5f}\")\n","\n","        total_recon /= len(train_loader)\n","\n","        # ================= VALIDATION (deterministic) =================\n","\n","        vae.eval()\n","        val_recon = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                img = batch[\"image\"].to(device)\n","\n","                mean, logvar = vae.encode(img)\n","                recon = vae.decode(mean)\n","\n","                val_recon += recon_loss(recon, img).item()\n","\n","        val_recon /= len(val_loader)\n","\n","        print(f\"\\nEpoch {epoch}\")\n","        print(f\"Train L1: {total_recon:.4f} | Val L1: {val_recon:.4f}\")\n","\n","        # ================= PREVIEW (deterministic) =================\n","\n","        with torch.no_grad():\n","            mean, logvar = vae.encode(preview)\n","            recon = vae.decode(mean)\n","\n","        vis = torch.cat([preview, recon], dim=0)\n","\n","        save_path = os.path.join(preview_dir, f\"epoch_{epoch}.png\")\n","        save_image((vis + 1) / 2, save_path, nrow=8)\n","\n","        show_grid(vis, title=f\"Epoch {epoch}\")\n","\n","        # ================= SAVE =================\n","\n","        if val_recon < best_val:\n","            best_val = val_recon\n","            torch.save(vae.state_dict(), best_model)\n","            print(\"New best model saved\")\n","\n","        torch.save({\n","            \"epoch\": epoch,\n","            \"model\": vae.state_dict(),\n","            \"optimizer\": optimizer.state_dict(),\n","            \"scaler\": scaler.state_dict(),\n","            \"best_val\": best_val\n","        }, ckpt_file)\n","\n","        print(\"Checkpoint saved\")\n","\n","    print(\"\\nTraining complete\")\n","\n","# ============================================================\n","# TEST\n","# ============================================================\n","\n","@torch.no_grad()\n","def test_vae(vae, test_loader,\n","             ckpt_path=\"./vae_outputs/vae_best.pth\"):\n","\n","    vae.load_state_dict(torch.load(ckpt_path, map_location=device))\n","    vae.eval()\n","\n","    batch = next(iter(test_loader))\n","    img = batch[\"image\"][:min(8, batch[\"image\"].size(0))].to(device)\n","\n","    mean, logvar = vae.encode(img)\n","    recon = vae.decode(mean)\n","\n","    show_grid(torch.cat([img, recon], dim=0),\n","              title=\"Test Reconstructions\")\n","\n","# ============================================================\n","# RUN\n","# ============================================================\n","\n","vae = VAE(z_channels=8).to(device)\n","\n","train_vae(vae, train_loader, val_loader)\n","\n","check_latent_stochasticity(vae, train_loader)\n","\n","test_vae(vae, test_loader)\n"]}]}