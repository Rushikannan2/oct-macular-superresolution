{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNccO0uAiNk/+p/Tcx+Bo2D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YKYJpjAxtNOe"},"outputs":[],"source":["# ============================================================\n","# 1. SETUP\n","# ============================================================\n","\n","import os\n","import cv2\n","import glob\n","import math\n","import random\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.utils import make_grid\n","import torchvision.transforms as T\n","\n","\n","# ============================================================\n","# RANDOM SEED\n","# ============================================================\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = True\n","\n","\n","seed_everything(42)\n","\n","\n","# ============================================================\n","# UPDATED DATASET PATHS (YOUR WINDOWS PATH)\n","# ============================================================\n","\n","ROOT = r\"D:\\Rushi_OCT_Diffusion\\CCDS_Split_10K-20260120T043900Z-3-001\\CCDS_Split_10K\"\n","\n","TRAIN_DIR = os.path.join(ROOT, \"train\")\n","VAL_DIR   = os.path.join(ROOT, \"val\")\n","TEST_DIR  = os.path.join(ROOT, \"test\")\n","\n","print(\"Train folder:\", TRAIN_DIR)\n","print(\"Val folder:\", VAL_DIR)\n","print(\"Test folder:\", TEST_DIR)\n","\n","if not os.path.exists(TRAIN_DIR):\n","    raise FileNotFoundError(\"Train directory not found\")\n","\n","if not os.path.exists(VAL_DIR):\n","    raise FileNotFoundError(\"Validation directory not found\")\n","\n","if not os.path.exists(TEST_DIR):\n","    raise FileNotFoundError(\"Test directory not found\")\n","\n","print(\"Dataset folders verified ✓\")\n","\n","\n","# ============================================================\n","# OUTPUT FOLDERS\n","# ============================================================\n","\n","OUT_ROOT = r\"D:\\Rushi_OCT_Diffusion\\oct_ldm_output\"\n","\n","DIR_SAMPLES      = os.path.join(OUT_ROOT, \"samples\")\n","DIR_CHECKPOINTS  = os.path.join(OUT_ROOT, \"checkpoints\")\n","DIR_PLOTS        = os.path.join(OUT_ROOT, \"plots\")\n","DIR_JSON         = os.path.join(OUT_ROOT, \"json\")\n","DIR_CSV          = os.path.join(OUT_ROOT, \"csv\")\n","\n","os.makedirs(DIR_SAMPLES, exist_ok=True)\n","os.makedirs(DIR_CHECKPOINTS, exist_ok=True)\n","os.makedirs(DIR_PLOTS, exist_ok=True)\n","os.makedirs(DIR_JSON, exist_ok=True)\n","os.makedirs(DIR_CSV, exist_ok=True)\n","\n","print(\"Output folders created at:\", OUT_ROOT)\n","\n","\n","# ============================================================\n","# DEVICE\n","# ============================================================\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","\n","\n","# ============================================================\n","# VISUALIZATION FUNCTION\n","# ============================================================\n","\n","def show_grid(tensor, nrow=8, title=\"\"):\n","    tensor = tensor.detach()\n","    grid = make_grid(tensor.clamp(-1, 1), nrow=nrow, normalize=False)\n","    grid = grid.permute(1, 2, 0).cpu().numpy()\n","\n","    plt.figure(figsize=(8, 8))\n","\n","    if grid.shape[-1] == 1:\n","        plt.imshow(grid[..., 0], cmap=\"gray\")\n","    else:\n","        plt.imshow(grid)\n","\n","    if title:\n","        plt.title(title)\n","\n","    plt.axis(\"off\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","print(\"OCT LDM environment ready ✓\")"]},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","# ============================================================\n","# DATASET CLASS (OPTIMIZED)\n","# ============================================================\n","\n","class OCTDataset(Dataset):\n","\n","    def __init__(self, root, image_size=512):\n","\n","        if not os.path.exists(root):\n","            raise FileNotFoundError(f\"Dataset folder not found: {root}\")\n","\n","        self.image_size = image_size\n","        self.paths = []\n","\n","        # FAST directory scan (faster than glob recursive)\n","        for dirpath, _, files in os.walk(root):\n","            for f in files:\n","                if f.lower().endswith(\".png\"):\n","                    self.paths.append(os.path.join(dirpath, f))\n","\n","        if not self.paths:\n","            raise RuntimeError(f\"No PNG files found in {root}\")\n","\n","        print(f\"Loaded {len(self.paths)} images from {root}\")\n","\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","\n","    def __getitem__(self, idx):\n","\n","        path = self.paths[idx]\n","        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            raise RuntimeError(f\"Failed to load image: {path}\")\n","\n","        h, w = img.shape\n","        t = self.image_size\n","\n","        scale = min(t / h, t / w)\n","        nh, nw = int(h * scale), int(w * scale)\n","\n","        img = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n","\n","        pad_h, pad_w = t - nh, t - nw\n","\n","        img = cv2.copyMakeBorder(\n","            img,\n","            pad_h // 2, pad_h - pad_h // 2,\n","            pad_w // 2, pad_w - pad_w // 2,\n","            cv2.BORDER_REFLECT_101\n","        )\n","\n","        img = torch.from_numpy(img).float().unsqueeze(0)\n","\n","        img = img / 127.5 - 1.0\n","\n","        patient = os.path.basename(os.path.dirname(path))\n","\n","        return {\n","            \"image\": img,\n","            \"patient\": patient,\n","            \"path\": path\n","        }\n","\n","\n","# ============================================================\n","# PATHS\n","# ============================================================\n","\n","DATA_ROOT = r\"D:\\Rushi_OCT_Diffusion\\CCDS_Split_10K-20260120T043900Z-3-001\\CCDS_Split_10K\"\n","\n","TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n","VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n","TEST_DIR  = os.path.join(DATA_ROOT, \"test\")\n","\n","print(\"Using dataset:\", DATA_ROOT)\n","\n","\n","# ============================================================\n","# LOADERS\n","# ============================================================\n","\n","def build_loader(path, batch_size=4, shuffle=False):\n","\n","    ds = OCTDataset(path)\n","\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=0,  # notebook safe\n","        pin_memory=torch.cuda.is_available()\n","    )\n","\n","\n","train_loader = build_loader(TRAIN_DIR, shuffle=True)\n","val_loader   = build_loader(VAL_DIR)\n","test_loader  = build_loader(TEST_DIR)\n","\n","print(\"\\nLoaders ready ✓\")\n","print(\"Train batches:\", len(train_loader))\n","print(\"Val batches:\", len(val_loader))\n","print(\"Test batches:\", len(test_loader))\n","\n","\n","# ============================================================\n","# SANITY TEST\n","# ============================================================\n","\n","batch = next(iter(train_loader))\n","\n","print(\"\\nBatch image shape:\", batch[\"image\"].shape)\n","print(\"Sample patients:\", batch[\"patient\"][:4])"],"metadata":{"id":"2SjUSxmWtgJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# ============================================================\n","# PATHS (ALIGNED WITH YOUR PREVIOUS STRUCTURE)\n","# ============================================================\n","\n","CHECKPOINT_DIR = r\"D:\\Rushi_OCT_Diffusion\\OCT_Local_pths\"\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","\n","VAE_SAVE_PATH = os.path.join(CHECKPOINT_DIR, \"vae_best.pth\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","\n","# ============================================================\n","# Utility: Safe GroupNorm\n","# ============================================================\n","\n","def norm_layer(ch):\n","    return nn.GroupNorm(min(32, ch), ch)\n","\n","\n","# ============================================================\n","# Residual Block\n","# ============================================================\n","\n","class ResBlock(nn.Module):\n","\n","    def __init__(self, ch):\n","        super().__init__()\n","        self.norm1 = norm_layer(ch)\n","        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)\n","        self.norm2 = norm_layer(ch)\n","        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        h = self.conv1(self.act(self.norm1(x)))\n","        h = self.conv2(self.act(self.norm2(h)))\n","        return x + h\n","\n","\n","# ============================================================\n","# Down Block\n","# ============================================================\n","\n","class DownBlock(nn.Module):\n","\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_ch, out_ch, 4, 2, 1)\n","        self.norm = norm_layer(out_ch)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        return self.act(self.norm(self.conv(x)))\n","\n","\n","# ============================================================\n","# Up Block\n","# ============================================================\n","\n","class UpBlock(nn.Module):\n","\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1)\n","        self.norm = norm_layer(out_ch)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        return self.act(self.norm(self.conv(x)))\n","\n","\n","# ============================================================\n","# VAE (CONSISTENT WITH YOUR LDM)\n","# ============================================================\n","\n","class VAE(nn.Module):\n","\n","    def __init__(self, im_channels=1, z_channels=16, base_ch=64):\n","        super().__init__()\n","\n","        # ---------------- Encoder ----------------\n","        self.conv_in = nn.Conv2d(im_channels, base_ch, 3, padding=1)\n","\n","        self.down1 = DownBlock(base_ch, base_ch * 2)\n","        self.res1  = ResBlock(base_ch * 2)\n","\n","        self.down2 = DownBlock(base_ch * 2, base_ch * 4)\n","        self.res2  = ResBlock(base_ch * 4)\n","\n","        self.down3 = DownBlock(base_ch * 4, base_ch * 4)\n","        self.res3  = ResBlock(base_ch * 4)\n","\n","        mid_ch = base_ch * 4\n","\n","        self.to_stats = nn.Conv2d(mid_ch, z_channels * 2, 3, padding=1)\n","\n","        # ---------------- Decoder ----------------\n","        self.from_latent = nn.Conv2d(z_channels, mid_ch, 3, padding=1)\n","\n","        self.res4 = ResBlock(mid_ch)\n","\n","        self.up1 = UpBlock(mid_ch, base_ch * 4)\n","        self.res5 = ResBlock(base_ch * 4)\n","\n","        self.up2 = UpBlock(base_ch * 4, base_ch * 2)\n","        self.res6 = ResBlock(base_ch * 2)\n","\n","        self.up3 = UpBlock(base_ch * 2, base_ch)\n","        self.res7 = ResBlock(base_ch)\n","\n","        self.norm_out = norm_layer(base_ch)\n","        self.conv_out = nn.Conv2d(base_ch, im_channels, 3, padding=1)\n","\n","\n","    # ============================================================\n","    # Encode\n","    # ============================================================\n","\n","    def encode(self, x):\n","        x = self.conv_in(x)\n","        x = self.res1(self.down1(x))\n","        x = self.res2(self.down2(x))\n","        x = self.res3(self.down3(x))\n","\n","        stats = self.to_stats(x)\n","        mean, logvar = torch.chunk(stats, 2, dim=1)\n","\n","        logvar = torch.clamp(logvar, -10, 10)\n","\n","        return mean, logvar\n","\n","\n","    # ============================================================\n","    # Reparameterization\n","    # ============================================================\n","\n","    def reparameterize(self, mean, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mean + eps * std\n","\n","\n","    # ============================================================\n","    # Decode\n","    # ============================================================\n","\n","    def decode(self, z):\n","        x = self.from_latent(z)\n","        x = self.res4(x)\n","\n","        x = self.res5(self.up1(x))\n","        x = self.res6(self.up2(x))\n","        x = self.res7(self.up3(x))\n","\n","        x = self.conv_out(F.silu(self.norm_out(x)))\n","\n","        return torch.tanh(x)\n","\n","\n","    # ============================================================\n","    # Forward\n","    # ============================================================\n","\n","    def forward(self, x):\n","        mean, logvar = self.encode(x)\n","        z = self.reparameterize(mean, logvar)\n","        recon = self.decode(z)\n","        return recon, mean, logvar\n","\n","\n","# ============================================================\n","# TEST (512 → 64 → 512)\n","# ============================================================\n","\n","if __name__ == \"__main__\":\n","\n","    vae = VAE().to(device)\n","\n","    x = torch.randn(1, 1, 512, 512).to(device)\n","\n","    recon, mean, logvar = vae(x)\n","\n","    print(\"Input:\", x.shape)\n","    print(\"Latent:\", mean.shape)   # should be [1, 16, 64, 64]\n","    print(\"Recon:\", recon.shape)"],"metadata":{"id":"N5ucGpXjtu8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","import glob\n","import cv2\n","import os\n","import torch\n","\n","\n","# ============================================================\n","# ROOT PATH (MATCHES YOUR PREVIOUS CODE)\n","# ============================================================\n","\n","DATA_ROOT = r\"D:\\Rushi_OCT_Diffusion\\CCDS_Split_10K-20260120T043900Z-3-001\\CCDS_Split_10K\"\n","\n","TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n","VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n","TEST_DIR  = os.path.join(DATA_ROOT, \"test\")\n","\n","if not os.path.exists(TRAIN_DIR):\n","    raise FileNotFoundError(\"Train directory not found\")\n","\n","if not os.path.exists(VAL_DIR):\n","    raise FileNotFoundError(\"Validation directory not found\")\n","\n","if not os.path.exists(TEST_DIR):\n","    raise FileNotFoundError(\"Test directory not found\")\n","\n","print(\"Dataset folders verified ✓\")\n","\n","\n","# ============================================================\n","# DATASET\n","# ============================================================\n","\n","class OCTDataset(Dataset):\n","\n","    def __init__(self, root, image_size=512):\n","\n","        self.paths = sorted(\n","            glob.glob(os.path.join(root, \"**\", \"*.png\"), recursive=True)\n","        )\n","\n","        self.image_size = image_size\n","\n","        if len(self.paths) == 0:\n","            raise RuntimeError(f\"No images found in {root}\")\n","\n","        print(f\"Loaded {len(self.paths)} images from {root}\")\n","\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","\n","    def __getitem__(self, idx):\n","\n","        path = self.paths[idx]\n","\n","        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            raise RuntimeError(f\"Failed to load image: {path}\")\n","\n","        # Fixed resolution (consistent with VAE symmetry)\n","        img = cv2.resize(\n","            img,\n","            (self.image_size, self.image_size),\n","            interpolation=cv2.INTER_AREA\n","        )\n","\n","        img = torch.from_numpy(img).float().contiguous()\n","\n","        img = img / 255.0\n","\n","        img = img.unsqueeze(0) * 2 - 1   # [-1, 1]\n","\n","        return {\"image\": img}\n","\n","\n","# ============================================================\n","# DATASETS\n","# ============================================================\n","\n","train_ds = OCTDataset(TRAIN_DIR)\n","val_ds   = OCTDataset(VAL_DIR)\n","test_ds  = OCTDataset(TEST_DIR)\n","\n","\n","# ============================================================\n","# LOADER SETTINGS (MATCH PREVIOUS PIPELINE)\n","# ============================================================\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","BATCH_SIZE = 4   # Matches your previous LDM setup\n","\n","loader_kwargs = dict(\n","    batch_size=BATCH_SIZE,\n","    num_workers=0,                 # Notebook-safe (as before)\n","    pin_memory=torch.cuda.is_available(),\n","    persistent_workers=False\n",")\n","\n","train_loader = DataLoader(train_ds, shuffle=True,  **loader_kwargs)\n","val_loader   = DataLoader(val_ds,   shuffle=False, **loader_kwargs)\n","test_loader  = DataLoader(test_ds,  shuffle=False, **loader_kwargs)\n","\n","print(\"\\nDataLoaders ready ✓\")\n","print(\"Batch size:\", BATCH_SIZE)\n","print(\"Train batches:\", len(train_loader))\n","print(\"Val batches:\", len(val_loader))\n","print(\"Test batches:\", len(test_loader))\n","\n","\n","# ============================================================\n","# SANITY CHECK\n","# ============================================================\n","\n","batch = next(iter(train_loader))\n","\n","print(\"\\nBatch image shape:\", batch[\"image\"].shape)"],"metadata":{"id":"xqegtqv_uEcI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from torchvision.utils import make_grid, save_image\n","import lpips\n","\n","\n","# ============================================================\n","# PATHS (MATCHES YOUR PREVIOUS LOCAL SETUP)\n","# ============================================================\n","\n","DATA_ROOT = r\"D:\\Rushi_OCT_Diffusion\\CCDS_Split_10K-20260120T043900Z-3-001\\CCDS_Split_10K\"\n","TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n","VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n","\n","OUTPUT_ROOT = r\"D:\\Rushi_OCT_Diffusion\\oct_ldm_output\"\n","VAE_OUT_DIR = os.path.join(OUTPUT_ROOT, \"vae_training\")\n","PREVIEW_DIR = os.path.join(VAE_OUT_DIR, \"previews\")\n","\n","os.makedirs(VAE_OUT_DIR, exist_ok=True)\n","os.makedirs(PREVIEW_DIR, exist_ok=True)\n","\n","\n","# ============================================================\n","# DEVICE\n","# ============================================================\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","\n","\n","# ============================================================\n","# SAFE NORMALIZATION\n","# ============================================================\n","\n","def normalize_batch(x):\n","\n","    if x.max() > 1.5:\n","        x = x / 127.5 - 1\n","    elif x.min() >= 0:\n","        x = x * 2 - 1\n","\n","    return x.clamp(-1, 1)\n","\n","\n","# ============================================================\n","# PERCEPTUAL LOSS (LPIPS)\n","# ============================================================\n","\n","lpips_loss = lpips.LPIPS(net=\"vgg\").to(device)\n","lpips_loss.eval()\n","\n","for p in lpips_loss.parameters():\n","    p.requires_grad = False\n","\n","\n","def perceptual_loss(pred, target):\n","\n","    pred3 = pred.repeat(1, 3, 1, 1)\n","    target3 = target.repeat(1, 3, 1, 1)\n","\n","    return lpips_loss(pred3, target3).mean()\n","\n","\n","# ============================================================\n","# LOSSES\n","# ============================================================\n","\n","def kl_loss(mean, logvar):\n","\n","    logvar = torch.clamp(logvar, -30, 20)\n","\n","    return -0.5 * torch.sum(\n","        1 + logvar - mean.pow(2) - logvar.exp(),\n","        dim=[1, 2, 3]\n","    ).mean()\n","\n","\n","def recon_loss(pred, target):\n","    return F.l1_loss(pred, target)\n","\n","\n","# ============================================================\n","# DISPLAY GRID\n","# ============================================================\n","\n","def show_grid(tensor, title=\"\"):\n","\n","    tensor = (tensor + 1) / 2\n","    tensor = torch.clamp(tensor, 0, 1)\n","\n","    grid = make_grid(tensor, nrow=4)\n","    grid = grid.cpu().permute(1, 2, 0).numpy()\n","\n","    plt.figure(figsize=(6, 6))\n","    plt.imshow(grid.squeeze(), cmap=\"gray\")\n","    plt.title(title)\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","\n","# ============================================================\n","# TRAIN FUNCTION\n","# ============================================================\n","\n","def train_vae(vae, train_loader, val_loader, epochs=150, lr=1e-4):\n","\n","    vae = vae.to(device)\n","\n","    optimizer = torch.optim.AdamW(vae.parameters(), lr=lr)\n","\n","    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n","\n","    preview = normalize_batch(\n","        next(iter(val_loader))[\"image\"][:4].to(device)\n","    )\n","\n","    best_val = float(\"inf\")\n","\n","    for epoch in range(1, epochs + 1):\n","\n","        torch.cuda.empty_cache()\n","\n","        beta = min(1e-4, epoch * 5e-6)   # KL warmup\n","        perc_weight = 0.005\n","\n","        vae.train()\n","        total_recon = 0\n","\n","        pbar = tqdm(train_loader, desc=f\"[TRAIN] Epoch {epoch}/{epochs}\")\n","\n","        for batch in pbar:\n","\n","            img = normalize_batch(batch[\"image\"].to(device))\n","\n","            optimizer.zero_grad()\n","\n","            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n","\n","                recon, mean, logvar = vae(img)\n","\n","                loss_l1 = recon_loss(recon, img)\n","                loss_p  = perceptual_loss(recon, img)\n","                loss_k  = kl_loss(mean, logvar)\n","\n","                loss = loss_l1 + perc_weight * loss_p + beta * loss_k\n","\n","            if not torch.isfinite(loss):\n","                print(\"NaN detected — stopping\")\n","                return\n","\n","            scaler.scale(loss).backward()\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            total_recon += loss_l1.item()\n","\n","            pbar.set_postfix(\n","                L1=f\"{loss_l1.item():.4f}\",\n","                KL=f\"{loss_k.item():.4f}\",\n","                beta=f\"{beta:.6f}\"\n","            )\n","\n","        total_recon /= len(train_loader)\n","\n","\n","        # ================= VALIDATION =================\n","\n","        vae.eval()\n","        val_recon = 0\n","\n","        with torch.no_grad():\n","\n","            for batch in val_loader:\n","\n","                img = normalize_batch(batch[\"image\"].to(device))\n","\n","                mean, logvar = vae.encode(img)\n","\n","                std = torch.exp(0.5 * logvar)\n","                eps = torch.randn_like(std)\n","                z = mean + std * eps\n","\n","                recon = vae.decode(z)\n","\n","                val_recon += recon_loss(recon, img).item()\n","\n","        val_recon /= len(val_loader)\n","\n","        print(f\"\\nEpoch {epoch}\")\n","        print(f\"Train L1: {total_recon:.4f} | Val L1: {val_recon:.4f}\")\n","\n","\n","        # ================= PREVIEW =================\n","\n","        with torch.no_grad():\n","\n","            mean, logvar = vae.encode(preview)\n","\n","            std = torch.exp(0.5 * logvar)\n","            eps = torch.randn_like(std)\n","            z = mean + std * eps\n","\n","            recon = vae.decode(z)\n","\n","        vis = torch.cat([preview, recon], dim=0)\n","\n","        save_image(\n","            (vis + 1) / 2,\n","            os.path.join(PREVIEW_DIR, f\"epoch_{epoch}.png\"),\n","            nrow=4\n","        )\n","\n","        show_grid(vis, title=f\"Epoch {epoch}\")\n","\n","\n","        # ================= SAVE BEST =================\n","\n","        if val_recon < best_val:\n","\n","            best_val = val_recon\n","\n","            torch.save(\n","                vae.state_dict(),\n","                os.path.join(VAE_OUT_DIR, \"vae_best.pth\")\n","            )\n","\n","            print(\"New best model saved\")\n","\n","    print(\"\\nVAE Training Complete\")\n","\n","\n","# ============================================================\n","# RUN\n","# ============================================================\n","\n","vae = VAE(z_channels=16).to(device)\n","train_vae(vae, train_loader, val_loader)"],"metadata":{"id":"VR-HO9_iuFPO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, glob, cv2\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.utils import save_image\n","from diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler\n","\n","\n","ROOT = r\"D:\\Rushi_OCT_Diffusion\\CCDS_Split_10K-20260120T043900Z-3-001\\CCDS_Split_10K\"\n","\n","TRAIN_PATH = os.path.join(ROOT, \"train\")\n","VAL_PATH   = os.path.join(ROOT, \"val\")\n","TEST_PATH  = os.path.join(ROOT, \"test\")\n","\n","CHECKPOINT_DIR = r\"D:\\Rushi_OCT_Diffusion\\OCT_Local_pths\"\n","BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"ldm_best.pth\")\n","CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"ldm_checkpoint.pth\")\n","VAE_PATH = os.path.join(CHECKPOINT_DIR, \"vae_best.pth\")\n","\n","PREVIEW_DIR = os.path.join(CHECKPOINT_DIR, \"epoch_samples\")\n","os.makedirs(PREVIEW_DIR, exist_ok=True)\n","\n","if not os.path.exists(TRAIN_PATH):\n","    raise FileNotFoundError(\"Train folder not found\")\n","\n","if not os.path.exists(VAE_PATH):\n","    raise FileNotFoundError(\"VAE checkpoint not found\")\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","\n","# ============================================================\n","# DATASET\n","# ============================================================\n","\n","class OCTDataset(Dataset):\n","\n","    def __init__(self, root, size=512):\n","        self.paths = sorted(\n","            glob.glob(os.path.join(root, \"**/*.png\"), recursive=True)\n","        )\n","        self.size = size\n","        print(f\"{root} -> {len(self.paths)} images\")\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, i):\n","        img = cv2.imread(self.paths[i], cv2.IMREAD_GRAYSCALE)\n","        img = cv2.resize(img, (self.size, self.size))\n","        img = torch.from_numpy(img).float() / 255.0\n","        img = img.unsqueeze(0) * 2 - 1\n","        return {\"image\": img}\n","\n","\n","train_loader = DataLoader(\n","    OCTDataset(TRAIN_PATH),\n","    batch_size=8,\n","    shuffle=True,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    OCTDataset(VAL_PATH),\n","    batch_size=8,\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","\n","# ============================================================\n","# VAE ARCHITECTURE (FOR LATENT ENCODING)\n","# ============================================================\n","\n","def norm_layer(ch):\n","    return nn.GroupNorm(min(32, ch), ch)\n","\n","\n","class ResBlock(nn.Module):\n","\n","    def __init__(self, ch):\n","        super().__init__()\n","        self.norm1 = norm_layer(ch)\n","        self.conv1 = nn.Conv2d(ch, ch, 3, 1, 1)\n","        self.norm2 = norm_layer(ch)\n","        self.conv2 = nn.Conv2d(ch, ch, 3, 1, 1)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        h = self.conv1(self.act(self.norm1(x)))\n","        h = self.conv2(self.act(self.norm2(h)))\n","        return x + h\n","\n","\n","class DownBlock(nn.Module):\n","\n","    def __init__(self, i, o):\n","        super().__init__()\n","        self.conv = nn.Conv2d(i, o, 4, 2, 1)\n","        self.norm = norm_layer(o)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        return self.act(self.norm(self.conv(x)))\n","\n","\n","class UpBlock(nn.Module):\n","\n","    def __init__(self, i, o):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose2d(i, o, 4, 2, 1)\n","        self.norm = norm_layer(o)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        return self.act(self.norm(self.conv(x)))\n","\n","\n","class VAE(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        b = 64\n","        z = 16\n","\n","        self.conv_in = nn.Conv2d(1, b, 3, 1, 1)\n","\n","        self.down1 = DownBlock(b, b * 2)\n","        self.res1 = ResBlock(b * 2)\n","\n","        self.down2 = DownBlock(b * 2, b * 4)\n","        self.res2 = ResBlock(b * 4)\n","\n","        self.down3 = DownBlock(b * 4, b * 4)\n","        self.res3 = ResBlock(b * 4)\n","\n","        self.to_stats = nn.Conv2d(b * 4, z * 2, 3, 1, 1)\n","\n","        self.from_latent = nn.Conv2d(z, b * 4, 3, 1, 1)\n","\n","        self.res4 = ResBlock(b * 4)\n","\n","        self.up1 = UpBlock(b * 4, b * 4)\n","        self.res5 = ResBlock(b * 4)\n","\n","        self.up2 = UpBlock(b * 4, b * 2)\n","        self.res6 = ResBlock(b * 2)\n","\n","        self.up3 = UpBlock(b * 2, b)\n","        self.res7 = ResBlock(b)\n","\n","        self.norm_out = norm_layer(b)\n","        self.conv_out = nn.Conv2d(b, 1, 3, 1, 1)\n","\n","    def encode(self, x):\n","        x = self.conv_in(x)\n","        x = self.res1(self.down1(x))\n","        x = self.res2(self.down2(x))\n","        x = self.res3(self.down3(x))\n","        mean, logvar = torch.chunk(self.to_stats(x), 2, 1)\n","        return mean, logvar.clamp(-10, 10)\n","\n","    def decode(self, z):\n","        x = self.from_latent(z)\n","        x = self.res4(x)\n","        x = self.res5(self.up1(x))\n","        x = self.res6(self.up2(x))\n","        x = self.res7(self.up3(x))\n","        return torch.tanh(self.conv_out(F.silu(self.norm_out(x))))\n","\n","\n","vae = VAE().to(device)\n","vae.load_state_dict(torch.load(VAE_PATH, map_location=device))\n","vae.eval().requires_grad_(False)\n","\n","\n","# ============================================================\n","# LDM (UNet + Scheduler)\n","# ============================================================\n","\n","unet = UNet2DModel(\n","    sample_size=64,\n","    in_channels=16,\n","    out_channels=16,\n","    layers_per_block=2,\n","    block_out_channels=(128, 256, 512, 512),\n","    down_block_types=(\n","        \"DownBlock2D\",\n","        \"AttnDownBlock2D\",\n","        \"AttnDownBlock2D\",\n","        \"AttnDownBlock2D\"\n","    ),\n","    up_block_types=(\n","        \"AttnUpBlock2D\",\n","        \"AttnUpBlock2D\",\n","        \"AttnUpBlock2D\",\n","        \"UpBlock2D\"\n","    )\n",").to(device)\n","\n","scheduler = DDPMScheduler(num_train_timesteps=1000)\n","ddim = DDIMScheduler(num_train_timesteps=1000)\n","\n","optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n","scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n","\n","start_epoch = 1\n","end_epoch = 100\n","best_loss = float(\"inf\")\n","\n","\n","if os.path.exists(CHECKPOINT_PATH):\n","    ck = torch.load(CHECKPOINT_PATH, map_location=device)\n","    unet.load_state_dict(ck[\"model\"])\n","    optimizer.load_state_dict(ck[\"optimizer\"])\n","    scaler.load_state_dict(ck[\"scaler\"])\n","    best_loss = ck[\"best_loss\"]\n","    start_epoch = ck[\"epoch\"] + 1\n","    print(f\"Resuming from epoch {ck['epoch']}\")\n","\n","\n","@torch.no_grad()\n","def sample_images(epoch):\n","\n","    unet.eval()\n","    ddim.set_timesteps(50)\n","\n","    z = torch.randn(4, 16, 64, 64, device=device)\n","\n","    for t in ddim.timesteps:\n","        noise_pred = unet(z, t).sample\n","        z = ddim.step(noise_pred, t, z).prev_sample\n","\n","    imgs = vae.decode(z)\n","    imgs = (imgs + 1) / 2\n","\n","    for i in range(4):\n","        save_image(\n","            imgs[i],\n","            os.path.join(PREVIEW_DIR, f\"epoch_{epoch:03d}_img{i+1}.png\")\n","        )\n","\n","\n","# ============================================================\n","# TRAINING LOOP\n","# ============================================================\n","\n","for epoch in range(start_epoch, end_epoch + 1):\n","\n","    unet.train()\n","    total_loss = 0\n","\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n","\n","        img = batch[\"image\"].to(device)\n","\n","        with torch.no_grad():\n","            mean, logvar = vae.encode(img)\n","            z = mean + torch.randn_like(mean) * torch.exp(0.5 * logvar)\n","\n","        noise = torch.randn_like(z)\n","        t = torch.randint(0, 1000, (z.size(0),), device=device)\n","\n","        xt = scheduler.add_noise(z, noise, t)\n","\n","        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n","            pred = unet(xt, t).sample\n","            loss = F.mse_loss(pred, noise)\n","\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    avg_train = total_loss / len(train_loader)\n","    print(\"Train Loss:\", avg_train)\n","\n","\n","    # ================= VALIDATION =================\n","\n","    unet.eval()\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","\n","            img = batch[\"image\"].to(device)\n","            mean, _ = vae.encode(img)\n","            z = mean\n","\n","            noise = torch.randn_like(z)\n","            t = torch.randint(0, 1000, (z.size(0),), device=device)\n","\n","            xt = scheduler.add_noise(z, noise, t)\n","            pred = unet(xt, t).sample\n","\n","            val_total += F.mse_loss(pred, noise).item()\n","\n","    avg_val = val_total / len(val_loader)\n","    print(\"Val Loss:\", avg_val)\n","\n","    sample_images(epoch)\n","\n","    if avg_val < best_loss:\n","        best_loss = avg_val\n","        torch.save(unet.state_dict(), BEST_MODEL_PATH)\n","\n","    torch.save({\n","        \"epoch\": epoch,\n","        \"model\": unet.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","        \"scaler\": scaler.state_dict(),\n","        \"best_loss\": best_loss\n","    }, CHECKPOINT_PATH)\n","\n","\n","print(\"Training Complete\")"],"metadata":{"id":"wbuagHlYvIpk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import glob\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from diffusers import UNet2DModel, DDIMScheduler\n","from pytorch_msssim import ssim\n","import lpips\n","\n","\n","# =======================\n","# PATH CONFIGURATION\n","# =======================\n","\n","DATA_ROOT = r\"D:\\Rushi_OCT_Diffusion\\CCDS_Split_10K-20260120T043900Z-3-001\\CCDS_Split_10K\"\n","CHECKPOINT_DIR = r\"D:\\Rushi_OCT_Diffusion\\OCT_Local_pths\"\n","\n","TEST_PATH = os.path.join(DATA_ROOT, \"test\")\n","BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"ldm_best.pth\")\n","VAE_PATH = os.path.join(CHECKPOINT_DIR, \"vae_best.pth\")\n","\n","EVAL_DIR = os.path.join(CHECKPOINT_DIR, \"Evaluation_Folder\")\n","os.makedirs(EVAL_DIR, exist_ok=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","\n","# =======================\n","# DATASET\n","# =======================\n","\n","class OCTDataset(Dataset):\n","\n","    def __init__(self, root, size=512):\n","        self.paths = sorted(\n","            glob.glob(os.path.join(root, \"**/*.png\"), recursive=True)\n","        )\n","        self.size = size\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, i):\n","        img = cv2.imread(self.paths[i], cv2.IMREAD_GRAYSCALE)\n","        img = cv2.resize(img, (self.size, self.size))\n","        img = torch.from_numpy(img).float() / 255.0\n","        img = img.unsqueeze(0) * 2 - 1\n","        return img\n","\n","\n","test_loader = DataLoader(\n","    OCTDataset(TEST_PATH),\n","    batch_size=8,\n","    shuffle=False\n",")\n","\n","\n","# =======================\n","# MODEL ARCHITECTURE\n","# =======================\n","\n","def norm_layer(ch):\n","    return nn.GroupNorm(min(32, ch), ch)\n","\n","\n","class ResBlock(nn.Module):\n","\n","    def __init__(self, ch):\n","        super().__init__()\n","        self.norm1 = norm_layer(ch)\n","        self.conv1 = nn.Conv2d(ch, ch, 3, 1, 1)\n","        self.norm2 = norm_layer(ch)\n","        self.conv2 = nn.Conv2d(ch, ch, 3, 1, 1)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        h = self.conv1(self.act(self.norm1(x)))\n","        h = self.conv2(self.act(self.norm2(h)))\n","        return x + h\n","\n","\n","class DownBlock(nn.Module):\n","\n","    def __init__(self, i, o):\n","        super().__init__()\n","        self.conv = nn.Conv2d(i, o, 4, 2, 1)\n","        self.norm = norm_layer(o)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        return self.act(self.norm(self.conv(x)))\n","\n","\n","class UpBlock(nn.Module):\n","\n","    def __init__(self, i, o):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose2d(i, o, 4, 2, 1)\n","        self.norm = norm_layer(o)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        return self.act(self.norm(self.conv(x)))\n","\n","\n","class VAE(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        b = 64\n","        z = 16\n","\n","        self.conv_in = nn.Conv2d(1, b, 3, 1, 1)\n","\n","        self.down1 = DownBlock(b, b * 2)\n","        self.res1 = ResBlock(b * 2)\n","\n","        self.down2 = DownBlock(b * 2, b * 4)\n","        self.res2 = ResBlock(b * 4)\n","\n","        self.down3 = DownBlock(b * 4, b * 4)\n","        self.res3 = ResBlock(b * 4)\n","\n","        self.to_stats = nn.Conv2d(b * 4, z * 2, 3, 1, 1)\n","\n","        self.from_latent = nn.Conv2d(z, b * 4, 3, 1, 1)\n","\n","        self.res4 = ResBlock(b * 4)\n","\n","        self.up1 = UpBlock(b * 4, b * 4)\n","        self.res5 = ResBlock(b * 4)\n","\n","        self.up2 = UpBlock(b * 4, b * 2)\n","        self.res6 = ResBlock(b * 2)\n","\n","        self.up3 = UpBlock(b * 2, b)\n","        self.res7 = ResBlock(b)\n","\n","        self.norm_out = norm_layer(b)\n","        self.conv_out = nn.Conv2d(b, 1, 3, 1, 1)\n","\n","    def encode(self, x):\n","        x = self.conv_in(x)\n","        x = self.res1(self.down1(x))\n","        x = self.res2(self.down2(x))\n","        x = self.res3(self.down3(x))\n","        m, l = torch.chunk(self.to_stats(x), 2, 1)\n","        return m, l\n","\n","    def decode(self, z):\n","        x = self.from_latent(z)\n","        x = self.res4(x)\n","        x = self.res5(self.up1(x))\n","        x = self.res6(self.up2(x))\n","        x = self.res7(self.up3(x))\n","        return torch.tanh(self.conv_out(F.silu(self.norm_out(x))))\n","\n","\n","vae = VAE().to(device)\n","vae.load_state_dict(torch.load(VAE_PATH, map_location=device))\n","vae.eval()\n","\n","\n","unet = UNet2DModel(\n","    sample_size=64,\n","    in_channels=16,\n","    out_channels=16,\n","    layers_per_block=2,\n","    block_out_channels=(128, 256, 512, 512),\n","    down_block_types=(\n","        \"DownBlock2D\",\n","        \"AttnDownBlock2D\",\n","        \"AttnDownBlock2D\",\n","        \"AttnDownBlock2D\"\n","    ),\n","    up_block_types=(\n","        \"AttnUpBlock2D\",\n","        \"AttnUpBlock2D\",\n","        \"AttnUpBlock2D\",\n","        \"UpBlock2D\"\n","    )\n",").to(device)\n","\n","unet.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n","unet.eval()\n","\n","scheduler = DDIMScheduler(num_train_timesteps=1000)\n","scheduler.set_timesteps(50)\n","\n","lpips_model = lpips.LPIPS(net=\"alex\").to(device)\n","\n","\n","# =======================\n","# GENERATION\n","# =======================\n","\n","@torch.no_grad()\n","def generate_samples(num):\n","\n","    z = torch.randn(num, 16, 64, 64, device=device)\n","\n","    for t in scheduler.timesteps:\n","        noise_pred = unet(z, t).sample\n","        z = scheduler.step(noise_pred, t, z).prev_sample\n","\n","    imgs = vae.decode(z)\n","    return (imgs + 1) / 2\n","\n","\n","# =======================\n","# EVALUATION LOOP\n","# =======================\n","\n","mse_total, psnr_total, ssim_total, lpips_total = 0, 0, 0, 0\n","count = 0\n","\n","for batch in test_loader:\n","\n","    real = (batch.to(device) + 1) / 2\n","    fake = generate_samples(real.size(0))\n","\n","    mse = F.mse_loss(fake, real).item()\n","    psnr = float(\"inf\") if mse == 0 else 10 * np.log10(1 / mse)\n","    ssim_val = ssim(fake, real, data_range=1).item()\n","\n","    lp = lpips_model(\n","        fake.repeat(1, 3, 1, 1),\n","        real.repeat(1, 3, 1, 1)\n","    ).mean().item()\n","\n","    mse_total += mse\n","    psnr_total += psnr\n","    ssim_total += ssim_val\n","    lpips_total += lp\n","    count += 1\n","\n","\n","mse_avg = mse_total / count\n","psnr_avg = psnr_total / count\n","ssim_avg = ssim_total / count\n","lpips_avg = lpips_total / count\n","\n","\n","with open(os.path.join(EVAL_DIR, \"metrics.txt\"), \"w\") as f:\n","    f.write(f\"MSE: {mse_avg}\\n\")\n","    f.write(f\"PSNR: {psnr_avg}\\n\")\n","    f.write(f\"SSIM: {ssim_avg}\\n\")\n","    f.write(f\"LPIPS: {lpips_avg}\\n\")\n","\n","\n","print(\"Evaluation Completed\")\n","print(\"MSE:\", mse_avg)\n","print(\"PSNR:\", psnr_avg)\n","print(\"SSIM:\", ssim_avg)\n","print(\"LPIPS:\", lpips_avg)"],"metadata":{"id":"oe3sTrNGv9qm"},"execution_count":null,"outputs":[]}]}